{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to the `cobar-week4` branch before you start!\n",
    "The `flygym` package is currently undergoing active development, and this week's exercises require updating the package in order to work properly. The main change is that the joint angles now come from behavioral data where the fly walked on a flat surface instead of a spherical treadmill. Follow these steps to integrate the new changes:\n",
    "\n",
    "1. Open your terminal.\n",
    "2. Navigate to the directory of your cloned `flygym` repository by executing the following command:\n",
    "   ```sh\n",
    "   cd flygym\n",
    "   ```\n",
    "3. To ensure you have the latest version of all branches, including the new changes on `cobar-week4`, first update your local repository with:\n",
    "   ```sh\n",
    "   git pull\n",
    "   ```\n",
    "4. Now, switch to the `cobar-week4` branch to access the updated features with:\n",
    "   ```sh\n",
    "   git checkout cobar-week4\n",
    "   ```\n",
    "(Note that this may change the simulation results for the exercises from previous weeks. For reproducibility, you may switch to the `main` branch before you run the notebooks from the previous weeks using `git checkout main`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision\n",
    "\n",
    "**Summary:** In this tutorial, we will build a simple model to control the fly to follow a moving sphere. By doing so, we will also demonstrate how one can create a custom arena.\n",
    "\n",
    "Animals typically navigate over rugged terrain to reach attractive objects (eg. potential mates, food sources) and to avoid repulsive features (eg. pheromones from predators) and obstacles. Animals use a hierarchical controller to achieve these goals: processing higher-order sensory signals, using them to select the next action, and translating these decisions into descending commands that drive lower-level motor systems. We aimed to simulate this sensorimotor hierarchy by adding vision and olfaction to NeuroMechFly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retina simulation\n",
    "\n",
    "A fly’s compound eye consists of ∼700–750 individual units called ommatidia arranged in a hexagonal pattern (see the left panel of the figure below from the [droso4schools project](https://droso4schools.wordpress.com/l4-enzymes/#5); see also [this article](https://azretina.sites.arizona.edu/node/789) from the Arizona Retina Project). To emulate this, we attached a color camera to each of our model’s compound eyes (top right panel). We then transformed each camera image into 721 bins, representing ommatidia. Based on previous studies, we assume a 270° combined azimuth for the fly’s field of view, with a ∼17° binocular overlap. Visual sensitivity has evolved to highlight ethologically relevant color spectra at different locations in the environment. Here, as an initial step toward enabling this heterogeneity in our model, we implemented yellow- and pale-type ommatidia—sensitive to the green and blue channels of images rendered by the physics simulator—randomly assigned at a 7:3 ratio (as reported in [Rister et al, 2013](https://pubmed.ncbi.nlm.nih.gov/23293281/)). Users can substitute the green and blue channel values with the desired light intensities sensed by yellow- and pale-type ommatidia to achieve more biorealistic chromatic vision.\n",
    "\n",
    "<img src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/vision.png?raw=true\" alt=\"rule_based\" width=\"800\"/>\n",
    "\n",
    "In NeuroMechFly, the main interface to interact with the fly is the [Retina class](https://neuromechfly.org/api_ref/vision.html#retina-simulation). It is embedded into the `NeuroMechFly` class as its `.retina` attribute if `enable_vision` is set to True in `NeuroMechFly.sim_params`. Here, let's reproduce the top right panel of the figure above by putting the fly into an arena with three pillars and simulating the fly's visual experience.\n",
    "\n",
    "To start, we do the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "from flygym.mujoco import Parameters\n",
    "from flygym.mujoco.arena import FlatTerrain\n",
    "from flygym.mujoco.examples.obstacle_arena import ObstacleOdorArena\n",
    "from flygym.mujoco.examples.turning_controller import HybridTurningNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-implemented an `ObstacleOdorArena` class with visual obstacles and an odor source. The details of this class is beyond the scope of this tutorial, but you can refer to it on the [FlyGym github repository](https://github.com/NeLy-EPFL/flygym/blob/main/flygym/mujoco/examples/obstacle_arena.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a simple arena\n",
    "flat_terrain_arena = FlatTerrain()\n",
    "\n",
    "# Then, we add visual and olfactory features on top of it\n",
    "arena = ObstacleOdorArena(\n",
    "    terrain=flat_terrain_arena,\n",
    "    obstacle_positions=np.array([(7.5, 0), (12.5, 5), (17.5, -5)]),\n",
    "    marker_size=0.5,\n",
    "    obstacle_colors=[(0.14, 0.14, 0.2, 1), (0.2, 0.8, 0.2, 1), (0.2, 0.2, 0.8, 1)],\n",
    "    user_camera_settings=((13, -18, 9), (np.deg2rad(65), 0, 0), 45),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the fly in it and simulate 500 timesteps so the fly can stand on the floor in a stable manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_sensor_placements = [\n",
    "    f\"{leg}{segment}\"\n",
    "    for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "    for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "]\n",
    "sim_params = Parameters(\n",
    "    render_playspeed=0.2,\n",
    "    render_camera=\"user_cam\",\n",
    "    enable_vision=True,\n",
    "    render_raw_vision=True,\n",
    "    enable_olfaction=True,\n",
    ")\n",
    "nmf = HybridTurningNMF(\n",
    "    sim_params=sim_params,\n",
    "    arena=arena,\n",
    "    spawn_pos=(13, -5, 0.2),\n",
    "    spawn_orientation=(0, 0, np.pi / 2 + np.deg2rad(70)),\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "\n",
    "for i in range(500):\n",
    "    obs, reward, terminated, truncated, info = nmf.step(np.zeros(2))\n",
    "    nmf.render()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3), tight_layout=True)\n",
    "ax.imshow(nmf._frames[-1])\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the intensities sensed by the fly's ommatidia from the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs[\"vision\"])\n",
    "print(\"Shape:\", obs[\"vision\"].shape)\n",
    "print(\"Data type:\", obs[\"vision\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a (2, 721, 2) array representing the light intensities sensed by the ommatidia. The values are normalized to [0, 1]. The first dimension is for the two eyes (left and right in that order). The second dimension is for the 721 ommatidia per eye. The third dimension is for the two color channels (yellow- and pale-type in that order). For each ommatidia, only one of the two numbers along the last dimension is nonzero. The yellow- and pale-type ommatidia are split at a 7:3 ratio. We can verify this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_idx = np.nonzero(obs[\"vision\"])\n",
    "unique_vals, val_counts = np.unique(nonzero_idx[2], return_counts=True)\n",
    "val_counts / val_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is array representation is not good for visualization. We can use the `hex_pxls_to_human_readable` method of the retina to convert it into a normal [0, 256) 8-bit RGB image that can be plotted. We set `color_8bit` to True to process the 8-bit color representation more efficiently and to return the output as an integer ranged from 0 to 255. We will further take the grayscale image (disregard yellow- vs pale-type ommatidia) by taking the maximum along the last dimension, ie. that of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_left = nmf.retina.hex_pxls_to_human_readable(obs[\"vision\"][0])\n",
    "vision_right = nmf.retina.hex_pxls_to_human_readable(obs[\"vision\"][1])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "axs[0].imshow(vision_left, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"Left eye\")\n",
    "axs[1].imshow(vision_right, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"Right eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `render_raw_vision` is set to True in the parameters, we can access the raw RGB vision through the `info` dictionary before pixels are binned into ommatidia: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "axs[0].imshow(info[\"raw_vision\"][0, :, :, :].astype(np.uint8))\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"Left eye\")\n",
    "axs[1].imshow(info[\"raw_vision\"][1, :, :, :].astype(np.uint8))\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"Right eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the ommatidia covering the blue and the green pillars seem to have a bimodal distribution in intensity. This is because the ommatidia are stochastically split into yellow- and pale-types, and they have different sensitivities to different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dynamic arena with a moving sphere\n",
    "\n",
    "The next step is to create a custom arena with a moving sphere. To do this, we will implement a `MovingObjArena` class that inherits from the `flygym.mujoco.arena.BaseArena` class. A complete, functioning implementation of this class is provided under `flygym.mujoco.examples.vision` on the [FlyGym repository](https://github.com/NeLy-EPFL/flygym/blob/main/flygym/mujoco/examples/vision.py). We start by defining some attributes in its `__init__` method:\n",
    "\n",
    "```Python\n",
    "class MovingObjArena(BaseArena):\n",
    "    \"\"\"Flat terrain with a hovering moving object.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    arena : mjcf.RootElement\n",
    "        The arena object that the terrain is built on.\n",
    "    ball_pos : Tuple[float,float,float]\n",
    "        The position of the floating object in the arena.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : Tuple[int, int]\n",
    "        The size of the terrain in (x, y) dimensions.\n",
    "    friction : Tuple[float, float, float]\n",
    "        Sliding, torsional, and rolling friction coefficients, by default\n",
    "        (1, 0.005, 0.0001)\n",
    "    obj_radius : float\n",
    "        Radius of the spherical floating object in mm.\n",
    "    obj_spawn_pos : Tuple[float,float,float]\n",
    "        Initial position of the object, by default (0, 2, 1).\n",
    "    move_direction : str\n",
    "        Which way the ball moves toward first. Can be \"left\", \"right\", or\n",
    "        \"random\".\n",
    "    move_speed : float\n",
    "        Speed of the moving object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: Tuple[float, float] = (300, 300),\n",
    "        friction: Tuple[float, float, float] = (1, 0.005, 0.0001),\n",
    "        obj_radius: float = 1,\n",
    "        init_ball_pos: Tuple[float, float] = (5, 0),\n",
    "        move_speed: float = 8,\n",
    "        move_direction: str = \"right\",\n",
    "    ):\n",
    "        self.init_ball_pos = (*init_ball_pos, obj_radius)\n",
    "        self.ball_pos = np.array(self.init_ball_pos, dtype=\"float32\")\n",
    "        self.friction = friction\n",
    "        self.move_speed = move_speed\n",
    "        self.curr_time = 0\n",
    "        self.move_direction = move_direction\n",
    "        if move_direction == \"left\":\n",
    "            self.y_mult = 1\n",
    "        elif move_direction == \"right\":\n",
    "            self.y_mult = -1\n",
    "        elif move_direction == \"random\":\n",
    "            self.y_mult = np.random.choice([-1, 1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid move_direction\")\n",
    "    \n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `root_element` attribute. The simulated world is represented as a tree of objects, each attached to a parent. For example, the eyes of the fly are attached to the head, which is in turn attached to the thorax — the base of the NeuroMechFly model. Note that this tree is merely a representation of objects and their relation to each other; there does not necessarily have to be a visual or anatomical link between the parent and child objects. For example, the base of the NeuroMechFly model — the thorax — is attached to the world, but the link between the thorax and the world is a free joint, meaning that it is free to move in all 6 degrees of freedom. The root element is the root of this tree.\n",
    "\n",
    "```Python\n",
    "        ...\n",
    "\n",
    "        self.root_element = mjcf.RootElement()\n",
    "\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will add the moving sphere. It will be attached to the root element:\n",
    "\n",
    "```Python\n",
    "        ...\n",
    "        \n",
    "        # Add ball\n",
    "        obstacle = self.root_element.asset.add(\n",
    "            \"material\", name=\"obstacle\", reflectance=0.1\n",
    "        )\n",
    "        self.root_element.worldbody.add(\n",
    "            \"body\", name=\"ball_mocap\", mocap=True, pos=self.ball_pos, gravcomp=1\n",
    "        )\n",
    "        self.object_body = self.root_element.find(\"body\", \"ball_mocap\")\n",
    "        self.object_body.add(\n",
    "            \"geom\",\n",
    "            name=\"ball\",\n",
    "            type=\"sphere\",\n",
    "            size=(obj_radius, obj_radius),\n",
    "            rgba=(0.0, 0.0, 0.0, 1),\n",
    "            material=obstacle,\n",
    "        )\n",
    "\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add some cameras so we can visualize the scene from different angles. This concludes the definition of our `__init__` method.\n",
    "\n",
    "```Python\n",
    "        ...\n",
    "\n",
    "        # Add camera\n",
    "        self.birdeye_cam = self.root_element.worldbody.add(\n",
    "            \"camera\",\n",
    "            name=\"birdeye_cam\",\n",
    "            mode=\"fixed\",\n",
    "            pos=(15, 0, 35),\n",
    "            euler=(0, 0, 0),\n",
    "            fovy=45,\n",
    "        )\n",
    "        self.birdeye_cam_zoom = self.root_element.worldbody.add(\n",
    "            \"camera\",\n",
    "            name=\"birdeye_cam_zoom\",\n",
    "            mode=\"fixed\",\n",
    "            pos=(15, 0, 20),\n",
    "            euler=(0, 0, 0),\n",
    "            fovy=45,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a `get_spawn_position` class. This is applies an offset to the user-defined fly spawn position. For example, if there is a stage in your arena that is 1 mm high, and you want to place the fly on this stage, then you might want to apply a transformation to the user-specified relative spawn position and return `rel_pos + np.array([0, 0, 1])` as the effective spawn position. In our case, we have a flat arena, so we will just return the spawn position and orientation as is:\n",
    "\n",
    "```Python\n",
    "    def get_spawn_position(self, rel_pos, rel_angle):\n",
    "        return rel_pos, rel_angle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arena also has a `step` method. Usually, in static arenas, this is left empty. However, since we want the sphere to move in our arena, we need to implement this method so the sphere is moved appropriately every step of the simulation:\n",
    "\n",
    "```Python\n",
    "    def step(self, dt, physics):\n",
    "        heading_vec = np.array([1, 2 * np.cos(self.curr_time * 3) * self.y_mult])\n",
    "        heading_vec /= np.linalg.norm(heading_vec)\n",
    "        self.ball_pos[:2] += self.move_speed * heading_vec * dt\n",
    "        physics.bind(self.object_body).mocap_pos = self.ball_pos\n",
    "        self.curr_time += dt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's implement a `reset` method:\n",
    "\n",
    "```Python\n",
    "    def reset(self, physics):\n",
    "        if self.move_direction == \"random\":\n",
    "            self.y_mult = np.random.choice([-1, 1])\n",
    "        self.curr_time = 0\n",
    "        self.ball_pos = np.array(self.init_ball_pos, dtype=\"float32\")\n",
    "        physics.bind(self.object_body).mocap_pos = self.ball_pos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual feature preprocessing\n",
    "\n",
    "We will preprocess the visual feature by computing the x-y position of the object on the retina along with its size relative to the whole retinal image. We do this by applying binary thresholding to the image and calculating its size and center of mass. This is a good example to once again showcase the benefit of encapsulating preprogrammed logic into the Markov Decision Process (implemented as a Gym environment). If you haven't, read the tutorial on [building a turning controller](https://neuromechfly.org/tutorials/turning.html) to see how this is done.\n",
    "\n",
    "Recall that in the `HybridTurningController`, we implemented the purple arrow in the following figure, encapsulating the CPG network and the sensory feedback-based correction rules:\n",
    "\n",
    "<img src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/mdp.png?raw=true\" alt=\"rule_based\" width=\"500\"/>\n",
    "\n",
    "Here, we will build yet another layer on top of `HybridTurningController`, implementing the aforementioned sensory preprocessing logic (cyan arrow) and encapsulating it inside the new MDP. As before, a complete, functioning implementation of this class is provided under `flygym.mujoco.examples.vision` on the [FlyGym repository](https://github.com/NeLy-EPFL/flygym/blob/main/flygym/mujoco/examples/vision.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining an `__init__` method. This time, we will specify the threshold used in the binary thresholding step. Any pixel darker than this number will be considered part of the black sphere. We will also define a decision interval $\\tau$: the turning signal is recomputed every $\\tau$ seconds. We will compute the center of mass (COM) of all ommatidia so that later when we need to compute the COM of the object (a masked subset of pixels), we can simply take the average of the COMs of the selected pixels. Finally, we will override the definition of the observation space with a 6-dimensional one (x, y positions of the object per side, plus the relative size of the object per side).\n",
    "\n",
    "```Python\n",
    "class VisualTaxis(HybridTurningNMF):\n",
    "    def __init__(self, obj_threshold=0.15, decision_interval=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.obj_threshold = obj_threshold\n",
    "        self.decision_interval = decision_interval\n",
    "        self.num_substeps = int(self.decision_interval / self.timestep)\n",
    "        self.visual_inputs_hist = []\n",
    "\n",
    "        self.coms = np.empty((self.retina.num_ommatidia_per_eye, 2))\n",
    "        for i in range(self.retina.num_ommatidia_per_eye):\n",
    "            mask = self.retina.ommatidia_id_map == i + 1\n",
    "            self.coms[i, :] = np.argwhere(mask).mean(axis=0)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1, shape=(6,))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement the visual preprocessing logic discussed above:\n",
    "\n",
    "```Python\n",
    "    def _process_visual_observation(self, raw_obs):\n",
    "        features = np.zeros((2, 3))\n",
    "        for i, ommatidia_readings in enumerate(raw_obs[\"vision\"]):\n",
    "            is_obj = ommatidia_readings.max(axis=1) < self.obj_threshold\n",
    "            is_obj_coords = self.coms[is_obj]\n",
    "            if is_obj_coords.shape[0] > 0:\n",
    "                features[i, :2] = is_obj_coords.mean(axis=0)\n",
    "            features[i, 2] = is_obj_coords.shape[0]\n",
    "        features[:, 0] /= self.retina.nrows  # normalize y_center\n",
    "        features[:, 1] /= self.retina.ncols  # normalize x_center\n",
    "        features[:, 2] /= self.retina.num_ommatidia_per_eye  # normalize area\n",
    "        return features.flatten()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `step` method, we will replace the raw observation with the output of the `_process_visual_observation` method. We will also record the retina images every time the simulation renders a frame for the recorded video. This way, we can visualize the retina images along with the recorded video later:\n",
    "\n",
    "```Python\n",
    "    def step(self, control_signal):\n",
    "        for _ in range(self.num_substeps):\n",
    "            raw_obs, _, _, _, _ = super().step(control_signal)\n",
    "            render_res = super().render()\n",
    "            if render_res is not None:\n",
    "                # record visual inputs too because they will be played in the video\n",
    "                self.visual_inputs_hist.append(raw_obs[\"vision\"].copy())\n",
    "        visual_features = self._process_visual_observation(raw_obs)\n",
    "        return visual_features, 0, False, False, {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement the `reset` method:\n",
    "\n",
    "```Python\n",
    "    def reset(self, seed=0, **kwargs):\n",
    "        raw_obs, _ = super().reset(seed=seed)\n",
    "        return self._process_visual_observation(raw_obs), {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a object tracking controller\n",
    "\n",
    "Now that we have implemented the arena and the new Gym environment, we just need to define the actual controller logic that outputs the 2D descending representation based on the extracted visual features. As a proof-of-concept, we have hand-tuned the following relationship:\n",
    "\n",
    "$$\n",
    "\\delta_i = \\begin{cases}\n",
    "\\min(\\max(k a_i + b, \\delta_\\text{min}), \\delta_\\text{max})   & \\text{if } A_i > A_\\text{thr} \\\\\n",
    "1  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\delta_i$ is the descending modulation signal on side $i$; $a_i$ is the azimuth of the object expressed as the deviation from the anterior edge of the eye's field of view, normalized by the horizontal field of view of the retina; $A_i$ is the relative size of the object on the arena; $k=-3$, $b=1$ are parameters describing the response curve; $\\delta_\\text{min}=0.2$, $\\delta_\\text{max}=1$ describe the range of the descending signal; $A_\\text{thr} = 1\\%$ is the threshold below which the object is considered unseen from the eye.\n",
    "\n",
    "Before we implement this in the main simulation loop, let's instantiate our arena and Gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.mujoco.examples.vision import MovingObjArena, VisualTaxis\n",
    "\n",
    "obj_threshold = 0.2\n",
    "decision_interval = 0.025\n",
    "contact_sensor_placements = [\n",
    "    f\"{leg}{segment}\"\n",
    "    for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "    for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "]\n",
    "arena = MovingObjArena()\n",
    "sim_params = Parameters(\n",
    "    render_camera=\"birdeye_cam\",\n",
    "    render_playspeed=0.5,\n",
    "    render_window_size=(800, 608),\n",
    "    enable_adhesion=True,\n",
    "    enable_vision=True,\n",
    ")\n",
    "nmf = VisualTaxis(\n",
    "    obj_threshold=obj_threshold,\n",
    "    decision_interval=decision_interval,\n",
    "    sim_params=sim_params,\n",
    "    arena=arena,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    intrinsic_freqs=np.ones(6) * 9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's check if this environment complies with the Gym interface. Despite a few warnings on design choices, no errors should be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the main simulation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ipsilateral_speed(deviation, is_found):\n",
    "    if not is_found:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.clip(1 - deviation * 3, 0.4, 1.2)\n",
    "\n",
    "\n",
    "num_substeps = int(decision_interval / sim_params.timestep)\n",
    "\n",
    "obs_hist = []\n",
    "deviations_hist = []\n",
    "control_signal_hist = []\n",
    "\n",
    "obs, _ = nmf.reset(seed=0)\n",
    "arena.reset(nmf.physics)\n",
    "\n",
    "for i in trange(140):\n",
    "    left_deviation = 1 - obs[1]\n",
    "    right_deviation = obs[4]\n",
    "    left_found = obs[2] > 0.01\n",
    "    right_found = obs[5] > 0.01\n",
    "    if not left_found:\n",
    "        left_deviation = np.nan\n",
    "    if not right_found:\n",
    "        right_deviation = np.nan\n",
    "    control_signal = np.array(\n",
    "        [\n",
    "            calc_ipsilateral_speed(left_deviation, left_found),\n",
    "            calc_ipsilateral_speed(right_deviation, right_found),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    obs, _, _, _, _ = nmf.step(control_signal)\n",
    "    obs_hist.append(obs)\n",
    "    deviations_hist.append([left_deviation, right_deviation])\n",
    "    control_signal_hist.append(control_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the recorded video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs\n",
    "nmf.save_video(\"./outputs/object_following.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `save_video_with_vision_insets` utility function to regenerate this video, but with insets at the bottom illustrating the visual experience of the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.mujoco.vision.visualize import save_video_with_vision_insets\n",
    "\n",
    "save_video_with_vision_insets(\n",
    "    nmf, \"./outputs/object_following_with_retina_images.mp4\", nmf.visual_inputs_hist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Optomotor response\n",
    "The [optomotor response](https://en.wikipedia.org/wiki/Optomotor_response) is an innate behavior commonly observed in insects and fish. It facilitates their movement in alignment with the direction of motion, allowing them to compensate for unforeseen disturbances such as gusts of air.\n",
    "\n",
    "In this exercise, we will simulate the process by which a fly determines the direction of moving gratings and adjusts its turning in response. This approach has been explored in previous studies, such as by [Strauss, Schuster, and Götz (1997)](https://doi.org/10.1242/jeb.200.9.1281)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gratings.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create an arena class for the grating stimulus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gratings(FlatTerrain):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n=18,\n",
    "        height=100,\n",
    "        distance=12,\n",
    "        ang_speed=1,\n",
    "        palette=((0, 0, 0, 1), (1, 1, 1, 1)),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Creates a circular arena with n cylinders to simulate a grating pattern.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of cylinders to create.\n",
    "        height : float\n",
    "            Height of the cylinders.\n",
    "        distance : float\n",
    "            Distance from the center of the arena to the center of the cylinders.\n",
    "        ang_speed : float\n",
    "            Angular speed of the cylinders.\n",
    "        palette : list of tuples\n",
    "            List of RGBA tuples to use as colors for the cylinders.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.height = height\n",
    "        self.ang_speed = ang_speed\n",
    "\n",
    "        self.cylinders = []\n",
    "        self.phase = 0\n",
    "        self.curr_time = 0\n",
    "\n",
    "        cylinder_material = self.root_element.asset.add(\n",
    "            \"material\", name=\"cylinder\", reflectance=0.1\n",
    "        )\n",
    "\n",
    "        #########################################################\n",
    "        # TODO: calculate the radius and the initial positions\n",
    "        # of the cylinders\n",
    "        init_positions = ...\n",
    "        radius = ...\n",
    "        #########################################################\n",
    "\n",
    "        self.init_positions = init_positions\n",
    "\n",
    "        for i, pos in enumerate(self.init_positions):\n",
    "            cylinder = self.root_element.worldbody.add(\n",
    "                \"body\",\n",
    "                name=f\"cylinder_{i}\",\n",
    "                mocap=True,\n",
    "                ##################################################\n",
    "                # TODO: set the position of the cylinder\n",
    "                pos=(..., ..., self.height / 2),\n",
    "                ##################################################\n",
    "            )\n",
    "\n",
    "            cylinder.add(\n",
    "                \"geom\",\n",
    "                type=\"cylinder\",\n",
    "                ##################################################\n",
    "                # TODO: set the size and color of the cylinder\n",
    "                size=(..., self.height / 2),\n",
    "                rgba=...,\n",
    "                ##################################################\n",
    "                material=cylinder_material,\n",
    "            )\n",
    "\n",
    "            self.cylinders.append(cylinder)\n",
    "\n",
    "        self.birdeye_cam = self.root_element.worldbody.add(\n",
    "            \"camera\",\n",
    "            name=\"birdeye_cam\",\n",
    "            mode=\"fixed\",\n",
    "            pos=(0, 0, 25),\n",
    "            euler=(0, 0, 0),\n",
    "            fovy=45,\n",
    "        )\n",
    "\n",
    "    def reset(self, physics):\n",
    "        \"\"\"Resets the position of the cylinders and the phase of the grating pattern.\"\"\"\n",
    "        self.phase = 0\n",
    "        self.curr_time = 0\n",
    "\n",
    "        ##################################################\n",
    "        # TODO: reset the position of the cylinders\n",
    "        for cylinder, pos in zip(self.cylinders, self.init_positions):\n",
    "            physics.bind(cylinder).mocap_pos = (\n",
    "                ...,    # x-coordinate here\n",
    "                ...,    # y-coordinate here\n",
    "                self.height / 2,\n",
    "            )\n",
    "        ##################################################\n",
    "\n",
    "    def step(self, dt, physics):\n",
    "        \"\"\"Steps the phase of the grating pattern and updates the position of the cylinders.\"\"\"\n",
    "\n",
    "        if self.curr_time % 1 < 1 / 2:\n",
    "            self.phase -= dt * self.ang_speed\n",
    "        else:\n",
    "            self.phase += dt * self.ang_speed\n",
    "\n",
    "        self.curr_time += dt\n",
    "\n",
    "        ##################################################\n",
    "        # TODO: update the position of the cylinders\n",
    "        positions = ...\n",
    "\n",
    "        for cylinder, pos in zip(self.cylinders, positions):\n",
    "            physics.bind(cylinder).mocap_pos = (\n",
    "                ...,    # x-coordinate here\n",
    "                ...,    # y-coordinate here\n",
    "                self.height / 2,\n",
    "            )\n",
    "        ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the arena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena = Gratings()\n",
    "\n",
    "contact_sensor_placements = [\n",
    "    f\"{leg}{segment}\"\n",
    "    for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "    for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "]\n",
    "sim_params = Parameters(\n",
    "    render_playspeed=0.2,\n",
    "    render_camera=\"birdeye_cam\",\n",
    "    enable_vision=True,\n",
    "    render_raw_vision=True,\n",
    "    enable_olfaction=True,\n",
    "    enable_adhesion=True,\n",
    "    render_window_size=(400, 400),\n",
    ")\n",
    "nmf = HybridTurningNMF(\n",
    "    sim_params=sim_params,\n",
    "    arena=arena,\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "\n",
    "nmf.render()\n",
    "\n",
    "for i in range(500):\n",
    "    obs, reward, terminated, truncated, info = nmf.step(np.zeros(2))\n",
    "    nmf.render()\n",
    "\n",
    "vision_left = nmf.retina.hex_pxls_to_human_readable(obs[\"vision\"][0, :, :])\n",
    "vision_right = nmf.retina.hex_pxls_to_human_readable(obs[\"vision\"][1, :, :])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3), tight_layout=True)\n",
    "axs[0].imshow(nmf._frames[-1])\n",
    "axs[1].imshow(vision_left, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].imshow(vision_right, cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "for ax, title in zip(axs, [\"Bird-eye view\", \"Left eye\", \"Right eye\"]):\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify array operations on visual inputs, we've defined a function designed to extract and reshape the central rectangular regions from hexagonal images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import crop_hex_to_rect\n",
    "\n",
    "imgs = crop_hex_to_rect(obs[\"vision\"])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "axs[0].imshow(imgs[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"Left eye\")\n",
    "axs[1].imshow(imgs[1], cmap=\"gray\", vmin=0, vmax=1)\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"Right eye\")\n",
    "\n",
    "print(\"Shape:\", imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intensities are relatively constant along the vertical direction. Let's compute the mean intensity for each column for the upper half of the image to further simplify the problem while retaining most of the information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_responses(visual_input):\n",
    "    im = crop_hex_to_rect(visual_input)\n",
    "    return crop_hex_to_rect(visual_input)[:, : im.shape[1] // 2, :].mean(1)\n",
    "\n",
    "\n",
    "column_responses = get_column_responses(obs[\"vision\"])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "axs[0].imshow(column_responses[0].reshape((1, -1)), cmap=\"gray\", vmin=0, vmax=1)\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"Left eye\")\n",
    "axs[1].imshow(column_responses[1].reshape((1, -1)), cmap=\"gray\", vmin=0, vmax=1)\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"Right eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hrc.png\" width=\"150\">\n",
    "\n",
    "Express the output of the HRC model in terms of the delayed intensities $I(x_i,t-\\tau), I(x_{i+1},t-\\tau)$ and the current intensities $I(x_i,t), I(x_{i+1},t)$.\n",
    "\n",
    "TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "\n",
    "def get_hrc_responses(delayed_intensity, current_intensity):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    delayed_intensity : np.ndarray\n",
    "        Delayed intensities with shape (2, 31).\n",
    "    current_intensity : np.ndarray\n",
    "        Current intensities with shape (2, 31).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        HRC responses with shape (2, 30).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #######################################################\n",
    "    # TODO: calculate the HRC outputs\n",
    "    # You might find the function `numpy.lib.stride_tricks.sliding_window_view` useful\n",
    "    outputs = ...\n",
    "    return outputs\n",
    "    #######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate two sinusoids, one moving to the left and one moving to the right, and make sure that the HRC outputs responses with opposite signs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_intensity = np.tile(np.cos(2 * np.pi * np.arange(31) / 31), (2, 1))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "for ax, shift in zip(axs, (-2, 2)):\n",
    "    current_intensity = np.roll(delayed_intensity, shift, axis=1)\n",
    "    ax.plot(delayed_intensity.T, color=\"C0\", label=r\"$I(x_i,t-\\tau)$\")\n",
    "    ax.plot(current_intensity.T, color=\"C1\", label=r\"$I(x_i,t)$\")\n",
    "    ax.set_title(\n",
    "        f\"HRC response = {get_hrc_responses(delayed_intensity, current_intensity).mean():.4f}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"$x_i$\")\n",
    "    ax.set_ylabel(\"Intensity\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::2], labels[::2], loc=\"upper center\", frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can simulate how the fly detect the motion direction and turn accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena.reset(nmf.physics)\n",
    "obs, info = nmf.reset(seed=10)\n",
    "\n",
    "n_delays = 4    # time delay = n_delays * 1 / vision_refresh_rate\n",
    "col_response_hist = [get_column_responses(obs[\"vision\"])] * n_delays\n",
    "visual_inputs_hist = []\n",
    "\n",
    "n_steps_per_vision_refresh = int(\n",
    "    1 / nmf.sim_params.vision_refresh_rate / nmf.sim_params.timestep\n",
    ")\n",
    "\n",
    "for i in trange(10000):\n",
    "    if i % n_steps_per_vision_refresh == 0:\n",
    "        # Get the delayed and current intensities\n",
    "        delayed_intensity = col_response_hist.pop(0)\n",
    "        current_intensity = get_column_responses(obs[\"vision\"])\n",
    "        col_response_hist.append(current_intensity)\n",
    "\n",
    "        hrc_response = get_hrc_responses(delayed_intensity, current_intensity).mean()\n",
    "\n",
    "        ######################################################################\n",
    "        # TODO: Apply turning descending signals of [-1.2, 1.2] or [1.2, -1.2]\n",
    "        # depending on the motion direction to make the fly turn towards the\n",
    "        # grating pattern.\n",
    "        action = ...\n",
    "        ######################################################################\n",
    "\n",
    "    obs = nmf.step(action)[0]\n",
    "    render_res = nmf.render()\n",
    "\n",
    "    if render_res is not None:\n",
    "        visual_inputs_hist.append(obs[\"vision\"].copy())\n",
    "\n",
    "save_video_with_vision_insets(nmf, \"outputs/optomotor.mp4\", visual_inputs_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we have learned how to\n",
    "- Create arenas with moving objects as visual stimuli\n",
    "- Simulate the motion detection process in the visual systems of flies\n",
    "- Transform visual information into motor actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flygym0.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
